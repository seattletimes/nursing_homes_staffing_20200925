{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to all the nursing home reports (enforcement letters and others)\n",
    "path = '/Volumes/files/COVID19/DSHS_Facility_Reports_Manuel/DSHS_ALTSA_Facility_Reports/reports_pdfs/NH_3333/'\n",
    "\n",
    "# Create a list of all the nursing home enforcement letters.\n",
    "enf_ltrs = pd.read_csv('../C_output_data/reports_metadata/reports_metadata_withPages_nhf.csv')\n",
    "enf_ltrs = enf_ltrs[enf_ltrs['rep_type']=='Enforcement Letter']\n",
    "enf_ltrs = enf_ltrs['pdf_name'].unique()\n",
    "enf_ltrs = pd.Series(enf_ltrs).sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', len(enf_ltrs), 'enforcement letters for nursing home facilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = ['Alderwood Park Health and Rehab (GG, CMP, CF) 6 14 18.pdf']\n",
    "\n",
    "print('open', smpl[0].replace('(', '\\(').replace(')', '\\)').replace(' ', '\\ '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe where all the data will be deposited.\n",
    "df_all_reports = pd.DataFrame(columns = ['pdf_name', 'rep_date', \n",
    "                                         'vendor_num', 'fed_num', 'aem_num', 'action', \n",
    "                                         'page', 'image_based', \n",
    "                                         'find_code', 'find_desc', \n",
    "                                         'wac', 'wac_mulct', 'total_mulct'])\n",
    "\n",
    "print('idx|pgs|rows|pdf_name')\n",
    "\n",
    "# The looooooong loop\n",
    "for index, pdf_name in smpl.iteritems():\n",
    "\n",
    "    pdf = pdfplumber.open(path + pdf_name)\n",
    "\n",
    "    print(index, '|', len(pdf.pages), '|', len(df_all_reports), '|', pdf_name)\n",
    "    \n",
    "    # For each page in the report:\n",
    "    for pg in pdf.pages:\n",
    "        print(pg.extract_text())\n",
    "        \n",
    "#             finds_list = pg.extract_text()\n",
    "#             finds_list = finds_list.split('(')\n",
    "#             finds_list = [f for f in finds_list  if re.match('^[A-Z]\\);', f)]\n",
    "#             if not finds_list:\n",
    "#                 find_code = np.nan\n",
    "#                 find_desc = np.nan\n",
    "#             else:\n",
    "#                 for find in finds_list:\n",
    "#                     find = find.split('.')[0] # Get rid of all text after the first period.\n",
    "#                     find = find.replace(')', '').replace('\\n', '') # Eliminate noise characters\n",
    "#                     find = find.split(';')\n",
    "#                     find_code = find[0].strip()\n",
    "#                     find_desc = find[1].strip()\n",
    "                    \n",
    "#             find_record = {'page':pg.page_number,\n",
    "#                            'find_code':find_code,\n",
    "#                            'find_desc':find_desc}\n",
    "#             df_finds = df_finds.append(find_record, ignore_index=True)\n",
    "\n",
    "            \n",
    "#             # ~~~~~ MULCTS: 'wac', 'wac_mulct', 'total_mulct'\n",
    "            \n",
    "#             # Find all the lines in the page that contain a $ sign. Start by \n",
    "#             # creating a list of all text lines in the page\n",
    "#             dollar_lines = pg.extract_text().split('\\n') \n",
    "#             dollar_lines = [line.strip() for line in  dollar_lines]\n",
    "#             # Reduce the previous list to only those lines that contain a $ sign\n",
    "#             dollar_lines = [line for line in dollar_lines if re.search('\\$', line)]\n",
    "            \n",
    "#             # ~~~~~ Total mulcted amount\n",
    "#             total_mulct_line = [line for line in dollar_lines if re.search('check', line)]\n",
    "#             if not total_mulct_line:\n",
    "#                 total_mulct = np.nan\n",
    "#             else:\n",
    "#                 # This list should only have one element, since there should only be \n",
    "#                 # one line with the total amount mulcted. Lets test for that:\n",
    "#                 assert len(total_mulct_line) == 1\n",
    "#                 try:\n",
    "#                     pattern = '\\$((\\d+(,\\s?|\\.)?)+\\d+)'\n",
    "#                     total_mulct = re.search(pattern, total_mulct_line[0]).group(1)\n",
    "#                     del(pattern)\n",
    "#                 except:\n",
    "#                     total_mulct = np.nan\n",
    "\n",
    "#             # ~~~~~ Breakdown of mulcted amount\n",
    "#             # Subset of those dollar_lines that START with a wac code\n",
    "#             wac_mulct_lines = [line for line in dollar_lines if re.search('^(WAC\\s?)?\\d+-\\d+-\\s?\\d+', line)]\n",
    "\n",
    "#             # If there are no lines with $ signs and WAC codes\n",
    "#             if not wac_mulct_lines:\n",
    "#                 wac = np.nan\n",
    "#                 wac_mulct = np.nan\n",
    "                \n",
    "#                 mulcts_record = {'page':pg.page_number,\n",
    "#                                  'wac':wac,\n",
    "#                                  'wac_mulct':wac_mulct,\n",
    "#                                  'total_mulct':total_mulct}\n",
    "#                 df_mulcts = df_mulcts.append(mulcts_record, ignore_index=True)\n",
    "#             # And if there are\n",
    "#             else:\n",
    "#                 for line in wac_mulct_lines:\n",
    "#                     # WAC code (Notice that here we don't require it to be at the beginnig)\n",
    "#                     pattern = '((WAC\\s?)?\\d+-\\d+-\\s?\\d+\\s?(\\(([0-9]|[a-z])\\)\\s?)*)'\n",
    "#                     wac = re.search(pattern, line).group(1)\n",
    "#                     del(pattern)\n",
    "#                     # Mulct\n",
    "#                     pattern = '\\$((\\d+(,|\\.)?)+\\d+)'\n",
    "#                     wac_mulct = re.search(pattern, line).group(1)\n",
    "#                     del(pattern)\n",
    "\n",
    "#                     mulcts_record = {'page':pg.page_number,\n",
    "#                                      'wac':wac,\n",
    "#                                      'wac_mulct':wac_mulct,\n",
    "#                                      'total_mulct':total_mulct}\n",
    "#                     df_mulcts = df_mulcts.append(mulcts_record, ignore_index=True)\n",
    "\n",
    "#     # Join the WAC mulcts data and the findings data into one single data frame\n",
    "#     df_one_report = df_finds.join(df_mulcts.set_index('page'), on='page', how='outer')\n",
    "\n",
    "#     # Add the report-wide data\n",
    "#     df_one_report['pdf_name'] = pdf_name\n",
    "#     df_one_report['image_based'] = False\n",
    "#     df_one_report['rep_date'] = rep_date\n",
    "#     df_one_report['action'] = action\n",
    "#     df_one_report['vendor_num'] = vendor_num\n",
    "#     df_one_report['fed_num'] = fed_num\n",
    "#     df_one_report['aem_num'] = aem_num\n",
    "\n",
    "#     # Rearrange columns according to the column order of 'df_all_reports' data frame\n",
    "#     df_one_report = df_one_report[df_all_reports.columns]\n",
    "\n",
    "#     # Attach 'df_one_report' to 'df_all_reports'\n",
    "#     df_all_reports = df_all_reports.append(df_one_report, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
