{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What this script does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the set of unique enforcement letters that will be out target, we dive into scraping festival using [Jeremy Singer-Vine](https://github.com/jsvine)'s amazing Python package, [PDF Plumber](https://github.com/jsvine/pdfplumber)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIST OF ENFORCEMENT LETTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have periodically ran bulk downloads of eforcement letters from the DSHS's [Nursing Home Facilities Locator](https://fortress.wa.gov/dshs/adsaapps/lookup/NHPubLookup.aspx), because the letters posted there change over time. The DSHS explained to us why that is the case:\n",
    "\n",
    "- On the first of each month an automatic script is ran that purges anything more than three years old and posts new enforcement letters.\n",
    "- In addition, when a facility closes down, all its documents automatically purge.\n",
    "- Sometimes, older enforcement letters (sometimes from previous years) are not present in the locator website, but will be posted later. Those older letters may be from nursing homes that had been going through a change of ownership. In that scenario, there would be a time where the locator would have no documents because the facility’s state license is officially “closed” (So all documents automatically purge). NH are the one facility type where even when they change ownership they still “own” their previous owners enforcement record. So the letters get reposted under the new licensee. There may be a delay between the reposting since that must be done manually.\n",
    "\n",
    "As a result, there each bulk download will have letters in commont with other bulk downloads, as well as letters that don't show up in other bulk downloads. Here we create a list of unique letters across all downloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/files/COVID19/Manuel_RCF_Data/State_DSHS/ALTSA_reports/'\n",
    "folders = ['NH_enforcement_letters_2020-03/',\n",
    "           'NH_enforcement_letters_2020-06-09/',\n",
    "           'NH_enforcement_letters_2020-06-24/',\n",
    "           'NH_enforcement_letters_2020-07-16/',\n",
    "           'NH_2020_Jan-Feb/individual_letters/']\n",
    "\n",
    "list_0 = listdir(path + folders[0])\n",
    "list_1 = listdir(path + folders[1])\n",
    "list_2 = listdir(path + folders[2])\n",
    "list_3 = listdir(path + folders[3])\n",
    "list_4 = listdir(path + folders[4])\n",
    "\n",
    "df_0 = pd.DataFrame(list_0, columns=['pdf_name'])\n",
    "df_0['folder'] = folders[0]\n",
    "\n",
    "df_1 = pd.DataFrame(list_1, columns=['pdf_name'])\n",
    "df_1['folder'] = folders[1]\n",
    "\n",
    "df_2 = pd.DataFrame(list_2, columns=['pdf_name'])\n",
    "df_2['folder'] = folders[2]\n",
    "\n",
    "df_3 = pd.DataFrame(list_3, columns=['pdf_name'])\n",
    "df_3['folder'] = folders[3]\n",
    "\n",
    "df_4 = pd.DataFrame(list_4, columns=['pdf_name'])\n",
    "df_4['folder'] = folders[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Advance Post Acute (G, CMP, CF) 4 6 18.pdf</td>\n",
       "      <td>NH_enforcement_letters_2020-03/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska Gardens (FP, Hx G, prior E, CMP, CF) 12...</td>\n",
       "      <td>NH_enforcement_letters_2020-03/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska Gardens (Hx D prior E) 7 16.pdf</td>\n",
       "      <td>NH_enforcement_letters_2020-03/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alaska Gardens (Hx F prior D) 8 9 18.pdf</td>\n",
       "      <td>NH_enforcement_letters_2020-03/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Gardens (Hx G, prior E) 11 16 18.pdf</td>\n",
       "      <td>NH_enforcement_letters_2020-03/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Warm_Beach_Care_Center_February_21_2017_letter...</td>\n",
       "      <td>NH_2020_Jan-Feb/individual_letters/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Washington_Veterans_Home_-_Retsil_February_10_...</td>\n",
       "      <td>NH_2020_Jan-Feb/individual_letters/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Willapa_Harbor_Health_and_Rehab_February_13_20...</td>\n",
       "      <td>NH_2020_Jan-Feb/individual_letters/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Willapa_Harbor_Health_and_Rehab_February_14_20...</td>\n",
       "      <td>NH_2020_Jan-Feb/individual_letters/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Willapa_Harbor_Health_and_Rehab_February_3_201...</td>\n",
       "      <td>NH_2020_Jan-Feb/individual_letters/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pdf_name  \\\n",
       "0           Advance Post Acute (G, CMP, CF) 4 6 18.pdf   \n",
       "1    Alaska Gardens (FP, Hx G, prior E, CMP, CF) 12...   \n",
       "2               Alaska Gardens (Hx D prior E) 7 16.pdf   \n",
       "3             Alaska Gardens (Hx F prior D) 8 9 18.pdf   \n",
       "4          Alaska Gardens (Hx G, prior E) 11 16 18.pdf   \n",
       "..                                                 ...   \n",
       "994  Warm_Beach_Care_Center_February_21_2017_letter...   \n",
       "995  Washington_Veterans_Home_-_Retsil_February_10_...   \n",
       "996  Willapa_Harbor_Health_and_Rehab_February_13_20...   \n",
       "997  Willapa_Harbor_Health_and_Rehab_February_14_20...   \n",
       "998  Willapa_Harbor_Health_and_Rehab_February_3_201...   \n",
       "\n",
       "                                  folder  \n",
       "0        NH_enforcement_letters_2020-03/  \n",
       "1        NH_enforcement_letters_2020-03/  \n",
       "2        NH_enforcement_letters_2020-03/  \n",
       "3        NH_enforcement_letters_2020-03/  \n",
       "4        NH_enforcement_letters_2020-03/  \n",
       "..                                   ...  \n",
       "994  NH_2020_Jan-Feb/individual_letters/  \n",
       "995  NH_2020_Jan-Feb/individual_letters/  \n",
       "996  NH_2020_Jan-Feb/individual_letters/  \n",
       "997  NH_2020_Jan-Feb/individual_letters/  \n",
       "998  NH_2020_Jan-Feb/individual_letters/  \n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_letters = pd.concat([df_0, df_1, df_2, df_3, df_4])\n",
    "df_letters = df_letters.drop_duplicates(subset=['pdf_name'], keep='first')\n",
    "df_letters = df_letters.reset_index(drop=True)\n",
    "\n",
    "df_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING FEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of reports that posed troubles\n",
    "\n",
    "# obstacles = [\n",
    "#     'Regency North Bend Direct care hrs WAC with CF.pdf',\n",
    "#     'Univeristy Place (FP, CMP, CF, Cond) 2 13 20.pdf',\n",
    "#     'Prestige Post Acute - Edmonds (IJ R, SP, SUB, CMP, CF) 4 5 18.pdf',\n",
    "#     'Forest Ridge (DCH) 4 24 18.pdf',\n",
    "#     'The Oaks at Timberline (Hx G, prior D, CMP, CF) 12 5 19.pdf',\n",
    "#     'Emerald Hills (GG, CMP, CF, Sub) 3 17.pdf', # Over $30K in cmp\n",
    "#     'Brookfield Cascadia (3 OOC, Sub, Hx, GG, Amend DCH, Prior G, CMP, CF) 4 25 18.pdf', # No date\n",
    "#     'Cristwood Nursing and Rehab (GG, CMP, CF) 6 21 18.pdf',\n",
    "#     'The Gardens on University ( HX GG, CMP, CF) 9 17.pdf',\n",
    "#     'Puyallup Nursing and Rehab (Hx D Prior F) 2 16 18.pdf', # Please remit a check for $XXXX.XX\n",
    "#     'Crestwood Health and Rehab Amended (DCH, Wac, CF) 10 6 17.pdf', # Space inside the fine amount: $35, 904.06 (Fixed)\n",
    "#     'Cheney Care Center Amended by hearing 2 (G, CMP, CF) 9 8 17.pdf', # Shows 2 cmp_total values. (Same value, repeated)\n",
    "#     'Alderwood Park Health and Rehab (GG, CMP, CF) 6 14 18.pdf', # An example that has ePOC and federal enforcement\n",
    "\n",
    "#     # Below this point are reports where we could scrap an individual mulct, but not its corresponding WAC code.\n",
    "#     'Brookfield Cascadia (Hx, GG, Amend DCH, Prior G, CMP, CF) 4 11 18.pdf', \n",
    "#     'Cashmere Care Center (D, DCH, CF) 7 31 19.pdf', \n",
    "#     'Crestwood (4OOC, DCH, prior G, prior D prior F, CMP, CF) 7 9 19.pdf', \n",
    "#     'Crestwood (5OOC DCH E prior E prior D prior D prior E) 11 14 19.pdf', \n",
    "#     'Crestwood (6OOC G, prior DCH E prior E prior D prior D prior E) 11 18 19.pdf', \n",
    "#     'Crestwood (Wac Only, CF, DCH) 8 17.pdf', 'Crestwood HRD quarterly fining 5 17.pdf', \n",
    "#     'Crestwood HRD quarterly fining Amended 5 17.pdf', 'Crestwood Health and Rehab  (DCH, Wac, CF) 10 6 17.pdf', \n",
    "#     'Crestwood Health and Rehab Amended (DCH, Wac, CF) 10 6 17.pdf', \n",
    "#     'Enumclaw Health & Rehab HRD Amended quarterly fining 5 17 .pdf', \n",
    "#     'Enumclaw Health & Rehab HRD quarterly fining 5 17 .pdf', \n",
    "#     'Forest Ridge (DCH) 11 4 19.pdf', 'Forest Ridge (DCH) 4 24 18.pdf', \n",
    "#     'Forest Ridge (F, DCH) 1 3 20.pdf', \n",
    "#     'Life Care of Port Townsend (DCH) 8 9 19.pdf', \n",
    "#     'McKay Healthcare and Rehab (DCH) 7 29 19.pdf', \n",
    "#     'Prestige Care - Burlington (G, DCH, CF, CMP) 10 1 19.pdf', \n",
    "#     'Regency North Bend (Direct Care hours, CF) 1 8 18.pdf', \n",
    "#     'Regency North Bend Direct care hrs WAC with CF.pdf', \n",
    "#     'Regency North Bend HRD quarterly fining 5 17.pdf', \n",
    "#     'Sunrise View (DCH, CF) 3 28 18.pdf', \n",
    "#     'Woodland DCH (CF) 9 6 19.pdf'\n",
    "# ]\n",
    "# obstacles = pd.Series(obstacles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loooooooooop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the dataframe where all the data will be deposited.\n",
    "df_all_reports = pd.DataFrame(columns = ['pdf_name', 'letter_date', \n",
    "                                         'survey_date', 'survey_type',\n",
    "                                         'vendor_num', 'fed_num', 'aem_num', 'action', \n",
    "                                         'page', 'image_based',\n",
    "                                         'fed_enforcement', 'nc_hist','epoc', 'state_rem', 'appeal_rights',\n",
    "                                         'find_code', 'find_desc', \n",
    "                                         'wac', 'cmp_item', 'cmp_total'])\n",
    "\n",
    "print('idx|pgs|rows|pdf_name')\n",
    "\n",
    "# For each enforcement letter:\n",
    "for index, row in df_letters.iterrows(): \n",
    "\n",
    "    pdf = pdfplumber.open(path + row['folder'] + row['pdf_name'])\n",
    "    print(index, '|', len(pdf.pages), '|', len(df_all_reports), '|', row['pdf_name'])\n",
    "    \n",
    "    # As we scrap each page, we will save the data we extract from them in these dataframes:\n",
    "    df_cmps = pd.DataFrame(columns = ['page', 'wac', 'cmp_item', 'cmp_total'])\n",
    "    df_sections = pd.DataFrame(columns = ['page','fed_enforcement', 'nc_hist', 'epoc', 'state_rem', 'appeal_rights'])\n",
    "    df_finds = pd.DataFrame(columns = ['page','find_code', 'find_desc'])\n",
    "\n",
    "    \n",
    "    # For each page in the report:\n",
    "    for pg in pdf.pages:\n",
    "        \n",
    "        # Exctract all the text in the page into a single variable\n",
    "        pg_txt = pg.extract_text()\n",
    "        \n",
    "        if not pg_txt:\n",
    "            new_record = {'pdf_name':row['pdf_name'],   #\n",
    "                          'letter_date':np.nan,\n",
    "                          'vendor_num':np.nan, \n",
    "                          'fed_num':np.nan, \n",
    "                          'aem_num':np.nan, \n",
    "                          'action':np.nan, \n",
    "                          'survey_date':np.nan, \n",
    "                          'survey_type':np.nan,\n",
    "                          'page':pg.page_number, # \n",
    "                          'image_based':True,    #\n",
    "                          'fed_enforcement':np.nan,\n",
    "                          'nc_hist':np.nan,\n",
    "                          'epoc':np.nan,\n",
    "                          'state_rem':np.nan,\n",
    "                          'appeal_rights':np.nan,\n",
    "                          'find_code':np.nan,\n",
    "                          'find_desc':np.nan,\n",
    "                          'wac':np.nan, \n",
    "                          'cmp_item':np.nan,\n",
    "                          'cmp_total':np.nan}\n",
    "            df_all_reports = df_all_reports.append(new_record, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # REPORT METADATA: 'letter_date, 'vendor_num', 'fed_num', 'aem_num', 'action'\n",
    "            # (These 5 data pieces are always on page 1)\n",
    "            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "            if pg.page_number == 1:\n",
    "\n",
    "                # ~~~~~ Data found on top area: 'letter_date, 'vendor_num', 'fed_num', 'aem_num'\n",
    "\n",
    "                top_area = pg.crop((pg.bbox[0], pg.bbox[1],\n",
    "                                     pg.bbox[2], pg.bbox[3]/3))\n",
    "                top_lines = top_area.extract_text().split('\\n')\n",
    "\n",
    "                # 'letter_date'\n",
    "                try:\n",
    "                    pattern = '^[A-Za-z]+\\s?\\d{1,2},\\s?\\d{4}'\n",
    "                    letter_date = [line for line in top_lines if re.match(pattern, line.strip())]\n",
    "                    letter_date = letter_date[0]\n",
    "                    del(pattern)\n",
    "                except:\n",
    "                    letter_date = np.nan\n",
    "\n",
    "                # 'vendor_num', 'fed_num', 'aem_num',\n",
    "                id_lines = [line for line in top_lines if re.match('^Vendor|AEM', line.strip())]\n",
    "                # Some times these data points are absent. For those cases:\n",
    "                if not id_lines: \n",
    "                    vendor_num = np.nan\n",
    "                    fed_num = np.nan\n",
    "                    aem_num = np.nan\n",
    "                # If we do find the data points:\n",
    "                else: \n",
    "                    # Vendor number\n",
    "                    try:\n",
    "                        vendor_num = id_lines[0].split('/')[0].strip()\n",
    "                        vendor_num = vendor_num.split(':')[-1].strip()\n",
    "                    except:\n",
    "                        vendor_num = np.nan\n",
    "                    # Fed number\n",
    "                    try:\n",
    "                        fed_num = id_lines[0].split('/')[1].strip()\n",
    "                        fed_num = fed_num.split(':')[-1].strip()\n",
    "                    except:\n",
    "                        fed_num = np.nan\n",
    "                    # AEM number\n",
    "                    try:\n",
    "                        aem_num = id_lines[1].split('#')[-1].strip()\n",
    "                    except:\n",
    "                        aem_num = np.nan\n",
    "\n",
    "\n",
    "                # ~~~~~ Data found on middle area: 'action'\n",
    "\n",
    "                middle_area = pg.crop((pg.bbox[0], pg.bbox[3]/3,\n",
    "                                       pg.bbox[2], pg.bbox[3]*2/3))\n",
    "                middle_lines = middle_area.extract_text().split('\\n')\n",
    "                middle_lines = [line.strip() for line in middle_lines]\n",
    "                middle_lines = [line for line in middle_lines if re.match('^[A-Z,\\s]+$', line)]\n",
    "                if middle_lines:\n",
    "                    action = ' '.join(middle_lines)\n",
    "                else:\n",
    "                    action = np.nan\n",
    "\n",
    "                    \n",
    "                    \n",
    "                # ~~~~~ 'survey_date' and 'survey_type'\n",
    "                \n",
    "                try:\n",
    "                    survey_txt = re.search('(On.*conducted an? .* at your facility)', pg_txt.replace('\\n','')).group(1)\n",
    "                    # There may have been more than one instance of the key phrase 'at your facility'.\n",
    "                    # Get just the first instance\n",
    "                    survey_txt = survey_txt.split('at your facility')[0]\n",
    "\n",
    "                    rgx = '((january|february|march|april|may|june|july|august|september|october|november|december)\\s\\d{1,2},?\\s\\d{4})'\n",
    "                    survey_date = re.search(rgx, survey_txt.lower()).group(1)\n",
    "\n",
    "                    rgx = 'conducted an? (.*)'\n",
    "                    survey_type = re.search(rgx, survey_txt.lower()).group(1)\n",
    "\n",
    "                except:\n",
    "                    survey_txt = np.nan\n",
    "                    survey_date = np.nan\n",
    "                    survey_type = np.nan\n",
    "\n",
    "                    \n",
    "            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # DETAILED DATA: Findings, federal enforcement, ePOC and fines\n",
    "            # (These data points can show up in any page, not just in page 1)\n",
    "            # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "            \n",
    "            # ~~~~~ Findings: 'find_code', 'find_desc'\n",
    "            \n",
    "            # Split the pate's text using a first parenthesis as the mark\n",
    "            finds_list = pg_txt.split('(')\n",
    "            # Reduce the created list to only the elements that start with this pattern: CAPITAL);\n",
    "            finds_list = [f for f in finds_list  if re.match('^[A-Z]\\);', f)]\n",
    "            if not finds_list:\n",
    "                find_code = np.nan\n",
    "                find_desc = np.nan\n",
    "            else:\n",
    "                # Select only the first find (If there are >1, probably from non-compliance history)\n",
    "                find = finds_list[0]\n",
    "                # Get rid of all text after the first period.\n",
    "                find = find.split('.')\n",
    "                find = find[0]\n",
    "                # Eliminate noise characters\n",
    "                find = find.replace(')', '').replace('\\n', '') \n",
    "                # Split using ';'\n",
    "                find = find.split(';')\n",
    "                # The first element of the new list is the finding code\n",
    "                find_code = find[0].strip()\n",
    "                # The second element of the new list is the finding description\n",
    "                find_desc = find[1].strip()\n",
    "                    \n",
    "            find_record = {'page':pg.page_number,\n",
    "                           'find_code':find_code,\n",
    "                           'find_desc':find_desc}\n",
    "            df_finds = df_finds.append(find_record, ignore_index=True)\n",
    "\n",
    "\n",
    "            \n",
    "            # ~~~~~ Sections\n",
    "            # - Federal enforcement\n",
    "            # - Electronic Plan of Correction (ePOC)\n",
    "            # - Non-compliance history\n",
    "            # - Appeal Rights\n",
    "\n",
    "            # For sections + fines, we will split the page text into lines using '\\n':\n",
    "            pg_lines = pg_txt.split('\\n')\n",
    "            pg_lines = [line.strip() for line in pg_lines]\n",
    "         \n",
    "            # For each section, determine if there is a line that indicates its presence in the report\n",
    "            fed_lines = [line for line in pg_lines if re.search('Federal Enforcement', line)]\n",
    "            epoc_lines = [line for line in pg_lines if re.search('Electronic Plan of Correction \\(ePOC\\)', line)]\n",
    "            history_lines = [line for line in pg_lines if re.search('History of Non-Compliance', line)]\n",
    "            remedy_lines = [line for line in pg_lines if re.search('State Remedies', line)]\n",
    "            appeal_lines = [line for line in pg_lines if re.search('Appeal Rights', line)]\n",
    "            \n",
    "            # Create a record for federal enforcement, nc history & epoc:\n",
    "            sections_record = {'page':pg.page_number,\n",
    "                               'fed_enforcement':len(fed_lines)>0,\n",
    "                               'nc_hist':len(history_lines)>0,\n",
    "                               'epoc':len(epoc_lines)>0,\n",
    "                               'state_rem':len(remedy_lines)>0,\n",
    "                               'appeal_rights':len(appeal_lines)>0\n",
    "                              }\n",
    "            df_sections = df_sections.append(sections_record, ignore_index=True)\n",
    "\n",
    "            \n",
    "            # ~~~~~ Fines: 'wac', 'cmp_item', 'cmp_total'\n",
    "            \n",
    "            # Look for any lines that contain a $ sign\n",
    "            dollar_lines = [line for line in pg_lines if re.search('\\$', line)]                        \n",
    "\n",
    "            # We now want to divide 'dollar_lines' list into two lists:\n",
    "            # 1- A list with only one line: the line that contains the aggregated amount of all fines\n",
    "            # 2- A list iwth all the lines that contain individual WAC fines\n",
    "            \n",
    "            # 1- Line that contain the aggregate fined amount\n",
    "            cmp_total_line = [line for line in dollar_lines if re.search('check', line)]\n",
    "            if not cmp_total_line:\n",
    "                cmp_total = np.nan\n",
    "            else:\n",
    "                # This list should only have one element, since there should only be \n",
    "                # one line with the total amount mulcted. Lets test for that:\n",
    "                assert len(cmp_total_line) == 1\n",
    "                try:\n",
    "                    pattern = '\\$((\\d+(,\\s?|\\.)?)+\\d+)'\n",
    "                    cmp_total = re.search(pattern, cmp_total_line[0]).group(1)\n",
    "                    del(pattern)\n",
    "                except:\n",
    "                    cmp_total = np.nan\n",
    "\n",
    "            # 2- Lines that contain individual WAC fines\n",
    "            # Subset of those dollar_lines that START with a wac code\n",
    "            cmp_item_lines = [line for line in dollar_lines if re.search('^(WAC\\s?)?\\d+-\\d+-\\s?\\d+', line)]\n",
    "\n",
    "            # If there are no lines with $ signs and WAC codes\n",
    "            if not cmp_item_lines:\n",
    "                wac = np.nan\n",
    "                cmp_item = np.nan\n",
    "                \n",
    "                cmp_record = {'page':pg.page_number,\n",
    "                                 'wac':wac,\n",
    "                                 'cmp_item':cmp_item,\n",
    "                                 'cmp_total':cmp_total}\n",
    "                df_cmps = df_cmps.append(cmp_record, ignore_index=True)\n",
    "            # And if there are\n",
    "            else:\n",
    "                for line in cmp_item_lines:\n",
    "                    # WAC code (Notice that here we don't require it to be at the beginnig)\n",
    "                    pattern = '((WAC\\s?)?\\d+-\\d+-\\s?\\d+\\s?(\\(([0-9]|[a-z])\\)\\s?)*)'\n",
    "                    wac = re.search(pattern, line).group(1)\n",
    "                    del(pattern)\n",
    "                    # CMP\n",
    "                    pattern = '\\$((\\d+(,|\\.)?)+\\d+)'\n",
    "                    cmp_item = re.search(pattern, line).group(1)\n",
    "                    del(pattern)\n",
    "\n",
    "                    cmp_record = {'page':pg.page_number,\n",
    "                                     'wac':wac,\n",
    "                                     'cmp_item':cmp_item,\n",
    "                                     'cmp_total':cmp_total}\n",
    "                    df_cmps = df_cmps.append(cmp_record, ignore_index=True)\n",
    "\n",
    "                    \n",
    "    # Consolidate df_finds, df_cmps & df_sections into a single data frame: df_one_report\n",
    "    df_one_report = df_finds.join(df_cmps.set_index('page'), on='page', how='outer').reset_index(drop=True)\n",
    "    df_one_report = df_one_report.join(df_sections.set_index('page'), on='page', how='outer').reset_index(drop=True)\n",
    "\n",
    "    # Add the report-wide data to df_one_report\n",
    "    df_one_report['pdf_name'] = row['pdf_name']\n",
    "    df_one_report['letter_date'] = letter_date\n",
    "    df_one_report['survey_date'] = survey_date\n",
    "    df_one_report['survey_type'] = survey_type\n",
    "    df_one_report['vendor_num'] = vendor_num\n",
    "    df_one_report['fed_num'] = fed_num\n",
    "    df_one_report['aem_num'] = aem_num\n",
    "    df_one_report['action'] = action\n",
    "    df_one_report['image_based'] = False\n",
    "    \n",
    "    # Rearrange columns according to the column order of 'df_all_reports' data frame\n",
    "    df_one_report = df_one_report[df_all_reports.columns]\n",
    "\n",
    "    # Attach 'df_one_report' to 'df_all_reports'\n",
    "    df_all_reports = df_all_reports.append(df_one_report, ignore_index=True)\n",
    "    del(df_one_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_reports.shape)\n",
    "print(df_all_reports.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING THE SCRAPED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all_reports.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAC codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first clean version of the code will include:\n",
    "- Title\n",
    "- Chapter\n",
    "- Section\n",
    "- Subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wac_clean_long'] = df['wac'].str.replace('WAC|\\s', '') # Don't need spaces, nor to be reminded they are *WAC* codes.\n",
    "df['wac_clean_long'] = df['wac_clean_long'].str.strip()\n",
    "df['wac_clean_long'] = df['wac_clean_long'].str.replace('97-97-', '97-')\n",
    "df['wac_clean_long'] = df['wac_clean_long'].str.replace('\\\\', '') # Only one case we could see with '\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create shorter version of the code, one that will not include the subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wac_clean_short'] = df['wac_clean_long'].str.extract('(\\d+-\\d+-\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Civil moneray penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform *cmp_item* and *cmp_total* into numeric type variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual cmp\n",
    "df['cmp_item_num'] = df['cmp_item'].str.strip().str.replace(',|\\s', '')\n",
    "df['cmp_item_num'] = df['cmp_item_num'].str.replace('1.500.00', '1500.00') # Just one case\n",
    "df['cmp_item_num'] = pd.to_numeric(df['cmp_item_num'])\n",
    "\n",
    "# Aggregate cmp\n",
    "df['cmp_total_num'] = df['cmp_total'].str.strip().str.replace(',|\\s', '')\n",
    "df['cmp_total_num'] = pd.to_numeric(df['cmp_total_num'])\n",
    "\n",
    "print('Total fines (from adding individual WAC fines of each report) =', df['cmp_item_num'].sum(skipna=True))\n",
    "print('Total fines (from adding only the aggregate fine of each report) =', df['cmp_total_num'].sum(skipna=True))\n",
    "print('So we have a slight discrepancy of', \n",
    "      df['cmp_total_num'].sum(skipna=True) - df['cmp_item_num'].sum(skipna=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform *letter_date* into a date-type variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistecy test: Does each enforcement letter have only one report date?\n",
    "temp = df[['pdf_name', 'letter_date']]\n",
    "temp = temp.drop_duplicates().reset_index(drop=True)\n",
    "assert len(temp) == df['pdf_name'].nunique()\n",
    "\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['letter_dt'] = pd.to_datetime(df['letter_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consistency test**: What are the oldest and earliest dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['letter_dt'].min())\n",
    "print(df['letter_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the earliest date is almost a century in the future. Obviously there must be something wrong with some years. Let's look for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['letter_dt'].dt.year.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['letter_date'].str.contains('2108|2107', na=False)]['pdf_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual review of those two PDF reports confirms our theory. We correct for that typo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['letter_dt'], axis=1)\n",
    "\n",
    "df['letter_dt'] = df['letter_date'].str.replace('2107', '2017').str.replace('2108', '2018')\n",
    "df['letter_dt'] = pd.to_datetime(df['letter_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['letter_dt'].dt.year.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['letter_dt'].min())\n",
    "print(df['letter_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform *survey_date* into a date type variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistecy test: Does each enforcement letter have only one report date?\n",
    "temp = df[['pdf_name', 'survey_date']]\n",
    "temp = temp.drop_duplicates().reset_index(drop=True)\n",
    "assert len(temp) == df['pdf_name'].nunique()\n",
    "\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survey_dt'] = pd.to_datetime(df['survey_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survey_date'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survey_dt'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consistency test**: What are the oldest and earliest dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['survey_dt'].min())\n",
    "print(df['survey_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the earliest date is almost a century in the future. Obviously there must be something wrong with some years. Let's look for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survey_dt'].dt.year.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['survey_date'].str.contains('2107|2108|2109', na=False)]['pdf_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual inspection of those letters shows that indeed these are typos, and consist in having the position of the '1' and the '0' in the year flipped. We correct for that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('survey_dt', axis=1)\n",
    "\n",
    "df['survey_dt'] = df['survey_date'].str.replace('2107','2017').str.replace('2108','2018').str.replace('2109','2019')\n",
    "df['survey_dt'] = pd.to_datetime(df['survey_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survey_dt'].dt.year.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['survey_dt'].min())\n",
    "print(df['survey_dt'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding WAC official definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the official WAC definitions\n",
    "df_wac = pd.read_csv('../C_output_data/wac_codes_df_t338c97.csv')\n",
    "\n",
    "# Join\n",
    "df = df.join(df_wac.set_index('ttl_chp_sec'), on='wac_clean_short', how='left')\n",
    "\n",
    "# Reorganize columns and eliminate obsolete ones\n",
    "df = df[['pdf_name', 'page', 'image_based', \n",
    "         'letter_dt', 'survey_dt', 'survey_type', 'vendor_num', 'fed_num', 'aem_num', 'action',\n",
    "         'fed_enforcement', 'nc_hist','epoc', 'state_rem', 'appeal_rights',\n",
    "         'find_code', 'find_desc', \n",
    "         'wac_clean_long', 'wac_clean_short', 'sub_chp_num', 'sub_chp_name', 'section', 'ttl_chp_sec_desc',\n",
    "         'cmp_item_num', 'cmp_total_num']]\n",
    "\n",
    "# Renaming columns\n",
    "df.columns = ['pdf_name', 'page', 'image_based', \n",
    "              'letter_dt', 'survey_dt', 'survey_type', 'vendor_num', 'fed_num', 'aem_num', 'action',\n",
    "              'fed_enforcement', 'nc_hist','epoc', 'state_rem', 'appeal_rights',\n",
    "              'finding_code', 'finding_desc', \n",
    "              'wac_long', 'wac_short', 'subchp_num', 'subchp_name', 'section', 'section_desc',\n",
    "              'cmp_item', 'cmp_agg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../C_output_data/scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA INTEGRITY REVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did all the reports make it through?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['pdf_name'].nunique() == len(df_letters)\n",
    "print('The consistency test above confirms that all the', len(df_letters), 'enforcement letters made it through.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many PDFs were image-based?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image = df[df['image_based']]\n",
    "\n",
    "print('Out of the', len(df_letters), 'enforcement letter PDFs, apparently only', \n",
    "      df_image['pdf_name'].nunique(), 'of them contained image-based pages.',\n",
    "      ' This does not necessarily mean that all pages in those PDFs are image-based, however. Let us confirm that:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the pdf_names that have at least one image-based page\n",
    "image_pdfs = df_image['pdf_name'].unique()\n",
    "len(image_pdfs)\n",
    "\n",
    "# Create a subset of df_all_reports that contains only the pdf_names that have at least one image-based page\n",
    "df_one_report = df[df['pdf_name'].isin(image_pdfs)]\n",
    "\n",
    "# Confirm all the pages in those PDFs are image-based\n",
    "assert df_one_report['image_based'].all()\n",
    "del(image_pdfs, df_one_report)\n",
    "\n",
    "print('Indeed, if a report is identified as image-based, then all of its pages are image-based.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved down copies of the image-based files to inspect visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "for pdf in df_image['pdf_name'].unique():\n",
    "    copyfile('/Volumes/files/COVID19/Manuel_RCF_Data/State_DSHS/ALTSA_reports/NH_3333/' + pdf, \n",
    "             '../D_Documents/DSHS/image_based_enforcement_letters/' + pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close look at those stratospheric fines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few unusual fines. Although they are rare—each only shows up once—they are stratospheric.\n",
    "\n",
    "They are so big, it is worth to take a close look at them. One test that is worth doing is whether the sum of the individual WAC fines adds up to the sum of the total fines for this particular group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cmp_item'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the cmp_item values and the number of times they appear through the reports\n",
    "mega_cmp_list = df['cmp_item'].value_counts(dropna=False).reset_index()\n",
    "mega_cmp_list.columns = ['cmp_item', 'freq']\n",
    "# Filter for only the unusual cmp_item amounts\n",
    "mega_cmp_list = mega_cmp_list[mega_cmp_list['freq'] == 1].reset_index(drop=True)\n",
    "# Reduce it to a series that contains those mega cmp\n",
    "mega_cmp_list = mega_cmp_list['cmp_item']\n",
    "mega_cmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of df with only the pdf_names that contain those unusual cmp\n",
    "df_mega_cmp = df[df['cmp_item'].isin(mega_cmp_list)]\n",
    "df_mega_cmp = df[df['pdf_name'].isin(df_mega_cmp['pdf_name'].unique())]\n",
    "df_mega_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency test: are all the unsual cmp_item values in this new data frame?\n",
    "assert mega_cmp_list.isin(df_mega_cmp['cmp_item']).all()\n",
    "\n",
    "# Are the sums of individual fines and total fines consistent for this group of mega fines?\n",
    "assert round(df_mega_cmp['cmp_item'].sum()) == round(df_mega_cmp['cmp_agg'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: A separate visual analysis these mega fines show that mostly correspond to a particular code violation: [WAC 388-97-1090(1)](https://apps.leg.wa.gov/wac/default.aspx?cite=388-97-1090), a Quality of Care related code that regulates the minimum hours of direct care per resident day (HDR) that a nursing home must provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From how many reports were we able to extract a total mulct amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[~df['cmp_agg'].isna()]\n",
    "\n",
    "print(len(temp))\n",
    "print(temp['pdf_name'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be one report that has 2 total amounts. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp['pdf_name'].value_counts().reset_index()\n",
    "x = temp[temp['pdf_name'] > 1].loc[0, 'index']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['pdf_name'] == x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['finding_code'].nunique())\n",
    "df['finding_code'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['finding_desc'].nunique())\n",
    "df['finding_desc'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ther are far more descriptions for a finding than codes. That is, a single finding code has typically multiple descriptions.\n",
    "\n",
    "Unless the official finding descriptions have changed over time, each code should only have one description. It seems that the problem here might arise from typos or because the algorithm picked up noise text when scraping for the descriptions. We should base our analysis in the finding codes instead of the descriptions and confirm that their definitions have not changed over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logically, all survey dates should precede their corresponding enforcement letter dates. Let's see if we can find instances where that is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df['letter_dt'] <= df['survey_dt'])]['pdf_name'].unique())\n",
    "df[(df['letter_dt'] <= df['survey_dt'])]#[['pdf_name', 'letter_dt','survey_dt']].drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
